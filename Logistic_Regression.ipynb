{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Assignment\n",
    "\n",
    "## Q1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
    "\n",
    "**Logistic Regression** is a statistical method used for binary classification problems. It predicts the probability of an outcome belonging to a particular category, typically represented as 0 or 1.\n",
    "\n",
    "### Differences from Linear Regression:\n",
    "1. **Output Type**: \n",
    "   - Linear Regression predicts continuous values.\n",
    "   - Logistic Regression predicts probabilities and classifies data into discrete categories (0 or 1).\n",
    "2. **Function Used**:\n",
    "   - Linear Regression uses a linear function: \\( y = \\beta_0 + \\beta_1x \\)\n",
    "   - Logistic Regression applies a **Sigmoid function** to convert the output into a probability.\n",
    "3. **Interpretation**:\n",
    "   - Linear Regression finds the best fit line.\n",
    "   - Logistic Regression finds a decision boundary to separate classes.\n",
    "\n",
    "## Q2. What is the mathematical equation of Logistic Regression?\n",
    "\n",
    "The equation of Logistic Regression is:\n",
    "\n",
    "\\[\n",
    "P(Y=1 | X) = \\frac{1}{1 + e^{- (\\beta_0 + \\beta_1X)}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P(Y=1 | X) \\) is the probability that output is 1.\n",
    "- \\( \\beta_0, \\beta_1 \\) are the model coefficients.\n",
    "- \\( X \\) is the input variable.\n",
    "- The **Sigmoid function** is used to map the output between 0 and 1.\n",
    "\n",
    "## Q3. Why do we use the Sigmoid function in Logistic Regression?\n",
    "\n",
    "The **Sigmoid function** converts any real-valued number into a probability between 0 and 1:\n",
    "\n",
    "\\[\n",
    "sigmoid(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "### Reasons for using Sigmoid:\n",
    "1. **Probability Interpretation**: The output is always between 0 and 1, making it suitable for classification.\n",
    "2. **Non-linearity**: Helps model complex relationships.\n",
    "3. **Decision Boundary**: Classifies data based on a probability threshold (e.g., 0.5).\n",
    "\n",
    "## Q4. What is the cost function of Logistic Regression?\n",
    "\n",
    "The **log loss function (Binary Cross-Entropy Loss)** is used:\n",
    "\n",
    "\\[\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the actual label (0 or 1).\n",
    "- \\( h_\\theta(x_i) \\) is the predicted probability.\n",
    "- \\( m \\) is the number of observations.\n",
    "\n",
    "This function penalizes incorrect predictions, ensuring that the model minimizes error.\n",
    "\n",
    "## Q5. What is Regularization in Logistic Regression? Why is it needed?\n",
    "\n",
    "**Regularization** is used to prevent overfitting by adding a penalty term to the cost function.\n",
    "\n",
    "### Types of Regularization:\n",
    "1. **L1 Regularization (Lasso)**: Adds the absolute value of coefficients.\n",
    "2. **L2 Regularization (Ridge)**: Adds the squared value of coefficients.\n",
    "\n",
    "### Why is it needed?\n",
    "- Prevents the model from memorizing noise.\n",
    "- Reduces model complexity.\n",
    "- Improves generalization on unseen data.\n",
    "\n",
    "## Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "\n",
    "| Regularization | Penalty Term | Effect |\n",
    "|--------------|-------------|--------|\n",
    "| **Lasso (L1)** | \\( \\lambda \\sum |\\beta_i| \\) | Shrinks some coefficients to zero, useful for feature selection. |\n",
    "| **Ridge (L2)** | \\( \\lambda \\sum \\beta_i^2 \\) | Reduces coefficient values but doesn’t make them zero. |\n",
    "| **Elastic Net** | \\( \\lambda_1 \\sum |\\beta_i| + \\lambda_2 \\sum \\beta_i^2 \\) | A mix of L1 and L2 regularization. |\n",
    "\n",
    "## Q7. When should we use Elastic Net instead of Lasso or Ridge?\n",
    "\n",
    "Elastic Net is preferred when:\n",
    "1. **There are correlated features** – Lasso may randomly select one and ignore others, while Elastic Net keeps them together.\n",
    "2. **Lasso selects too few features** – Elastic Net balances selection and shrinkage.\n",
    "\n",
    "## Q8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
    "\n",
    "- **Higher λ** → More regularization → Smaller coefficients → Simpler model.\n",
    "- **Lower λ** → Less regularization → More complex model.\n",
    "- **Too high λ** → Underfitting.\n",
    "- **Too low λ** → Overfitting.\n",
    "\n",
    "## Q9. What are the key assumptions of Logistic Regression?\n",
    "1. **Independent observations** – No multicollinearity among predictors.\n",
    "2. **Linearity in log-odds** – Relationship between features and log-odds is linear.\n",
    "3. **No missing values** – Data should be complete.\n",
    "\n",
    "## Q10. What are some alternatives to Logistic Regression for classification tasks?\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Support Vector Machines (SVM)\n",
    "- Neural Networks\n",
    "- Naïve Bayes\n",
    "\n",
    "## Q11. What are Classification Evaluation Metrics?\n",
    "1. **Accuracy**\n",
    "2. **Precision, Recall, F1-score**\n",
    "3. **ROC-AUC Curve**\n",
    "4. **Confusion Matrix**\n",
    "\n",
    "## Q12. How does class imbalance affect Logistic Regression?\n",
    "- Leads to biased predictions towards the majority class.\n",
    "- Solutions: **SMOTE, Class weights adjustment, Threshold tuning**.\n",
    "\n",
    "## Q13. What is Hyperparameter Tuning in Logistic Regression?\n",
    "- Adjusting parameters like **regularization strength (λ)** and **solver** to improve performance.\n",
    "- Done using **GridSearchCV** or **RandomizedSearchCV**.\n",
    "\n",
    "## Q14. What are different solvers in Logistic Regression? Which one should be used?\n",
    "| Solver | Type | When to Use |\n",
    "|--------|------|------------|\n",
    "| **lbfgs** | Quasi-Newton | Best for small datasets |\n",
    "| **liblinear** | Coordinate Descent | Good for L1 regularization |\n",
    "| **saga** | Stochastic Gradient Descent | Large datasets |\n",
    "| **newton-cg** | Newton’s Method | L2 regularization |\n",
    "\n",
    "## Q15. How is Logistic Regression extended for multiclass classification?\n",
    "- **One-vs-Rest (OvR)** – Trains one model per class.\n",
    "- **Softmax Regression (Multinomial)** – Generalizes Logistic Regression for multiple classes.\n",
    "\n",
    "## Q16. Advantages and Disadvantages of Logistic Regression\n",
    "**Advantages:**\n",
    "- Simple and interpretable.\n",
    "- Works well for small datasets.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Assumes linear relationship in log-odds.\n",
    "- Struggles with non-linearly separable data.\n",
    "\n",
    "## Q17. Use Cases of Logistic Regression\n",
    "- Spam detection\n",
    "- Medical diagnosis (e.g., cancer detection)\n",
    "- Credit scoring\n",
    "\n",
    "## Q18. Difference between Softmax Regression and Logistic Regression\n",
    "- **Logistic Regression**: Used for binary classification.\n",
    "- **Softmax Regression**: Used for multiclass classification.\n",
    "\n",
    "## Q19. One-vs-Rest (OvR) vs Softmax for Multiclass Classification\n",
    "- **OvR**: Suitable for unbalanced data.\n",
    "- **Softmax**: Preferred for balanced datasets.\n",
    "\n",
    "## Q20. Interpreting Coefficients in Logistic Regression\n",
    "- **Positive coefficient**: Increases probability of class 1.\n",
    "- **Negative coefficient**: Decreases probability of class 1.\n",
    "- **Magnitude**: Larger values have a stronger impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic \n",
    "Regression, and prints the model accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-1\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with L1 regularization (Lasso): 0.97\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using \n",
    "and print the model accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-2\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression with L1 regularization (Lasso)\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy with L1 regularization (Lasso): {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with L2 regularization (Ridge): 0.97\n",
      "Model Coefficients:\n",
      "[[-4.70615115e-01  6.52347631e-01  1.15785093e+00 -5.93992954e-01\n",
      "  -2.40023206e-02  1.22893828e-01  1.34792600e+00  1.21129337e-01\n",
      "  -3.06783773e-01 -7.98210594e-02 -1.51671435e-01  7.16848941e-01\n",
      "   1.41153994e-02]\n",
      " [ 8.05008446e-01 -1.09848675e+00 -8.53073630e-01  2.77643826e-01\n",
      "   2.95991312e-03  6.31546246e-02  4.73153950e-01  2.72080856e-01\n",
      "   6.91070110e-01 -1.78399238e+00  6.66509385e-01  3.35795670e-01\n",
      "  -1.20384492e-02]\n",
      " [-2.59133931e-01  6.52310177e-01  1.20611743e-01  8.04321890e-02\n",
      "   1.92407495e-02 -5.19182377e-01 -1.70239118e+00 -1.24123952e-01\n",
      "  -7.08593521e-01  1.03220422e+00 -4.64782177e-01 -1.23780486e+00\n",
      "  -5.56613338e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using \n",
    "LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
    "'''\n",
    "'''\n",
    "Answer:-3\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression with L2 regularization (Ridge)\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy with L2 regularization (Ridge): {accuracy:.2f}')\n",
    "\n",
    "# Print the model coefficients\n",
    "print('Model Coefficients:')\n",
    "print(model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with Elastic Net Regularization: 0.75\n",
      "Model Coefficients:\n",
      "[[-6.21567258e-03 -1.92336274e-03 -1.10466353e-03 -1.59779675e-02\n",
      "  -3.81752725e-02 -1.79947366e-04  8.71021186e-04 -3.20892685e-04\n",
      "  -2.08368407e-04 -3.17362120e-03 -2.81673039e-04 -1.49757684e-04\n",
      "   5.68278026e-03]\n",
      " [ 3.99992471e-03 -1.14389124e-03  5.98151297e-04  9.83853260e-03\n",
      "   2.99171678e-02  1.53344756e-03  2.24083732e-03  3.36831266e-05\n",
      "   1.25462532e-03 -5.40322349e-03  1.00319777e-03  2.68213354e-03\n",
      "  -4.43030562e-03]\n",
      " [ 2.17418785e-03  3.09605447e-03  4.64952220e-04  6.09786316e-03\n",
      "   8.21655057e-03 -1.31196361e-03 -3.15332479e-03  2.45655403e-04\n",
      "  -1.00470418e-03  8.61843399e-03 -6.79970580e-04 -2.49083927e-03\n",
      "  -1.21118997e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
    "'''\n",
    "'''\n",
    "Answer:-4\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression with Elastic Net Regularization\n",
    "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy with Elastic Net Regularization: {accuracy:.2f}')\n",
    "\n",
    "# Print the model coefficients\n",
    "print('Model Coefficients:')\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with multi_class=\"ovr\": 0.97\n",
      "Model Coefficients:\n",
      "[[-4.70615115e-01  6.52347631e-01  1.15785093e+00 -5.93992954e-01\n",
      "  -2.40023206e-02  1.22893828e-01  1.34792600e+00  1.21129337e-01\n",
      "  -3.06783773e-01 -7.98210594e-02 -1.51671435e-01  7.16848941e-01\n",
      "   1.41153994e-02]\n",
      " [ 8.05008446e-01 -1.09848675e+00 -8.53073630e-01  2.77643826e-01\n",
      "   2.95991312e-03  6.31546246e-02  4.73153950e-01  2.72080856e-01\n",
      "   6.91070110e-01 -1.78399238e+00  6.66509385e-01  3.35795670e-01\n",
      "  -1.20384492e-02]\n",
      " [-2.59133931e-01  6.52310177e-01  1.20611743e-01  8.04321890e-02\n",
      "   1.92407495e-02 -5.19182377e-01 -1.70239118e+00 -1.24123952e-01\n",
      "  -7.08593521e-01  1.03220422e+00 -4.64782177e-01 -1.23780486e+00\n",
      "  -5.56613338e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q5. Write a Python program to train a Logistic Regression model for multiclass classification using \n",
    "multi_class='ovr'.\n",
    "'''\n",
    "'''\n",
    "Answer:-5\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression for multiclass classification using one-vs-rest (ovr)\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy with multi_class=\"ovr\": {accuracy:.2f}')\n",
    "\n",
    "# Print the model coefficients\n",
    "print('Model Coefficients:')\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best Cross-Validation Accuracy: 0.95\n",
      "Test Set Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic \n",
    "Regression. Print the best parameters and accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-6\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Titanic_train.csv')\n",
    "\n",
    "# Drop columns that are not needed\n",
    "data = data.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop(columns=['Survived'])\n",
    "y = data['Survived']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy with Stratified K-Fold Cross-Validation: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the \n",
    "average accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-7\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Define the Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the model using Stratified K-Fold Cross-Validation\n",
    "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy\n",
    "average_accuracy = np.mean(scores)\n",
    "print(f'Average Accuracy with Stratified K-Fold Cross-Validation: {average_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7483\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its \n",
    "accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-8\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Convert categorical to numerical\n",
    "\n",
    "df = df[features + ['Survived']].dropna()  # Drop rows with missing values\n",
    "\n",
    "X = df[features]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': np.float64(0.046415888336127774)}\n",
      "Best Cross-Validation Score: 0.7977\n",
      "Test Set Accuracy: 0.7989\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in \n",
    "Logistic Regression. Print the best parameters and accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-9\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Preprocessing\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "\n",
    "# Drop columns that are not needed for the model\n",
    "df.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "# Convert categorical 'Sex' to numerical\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 4, 40),  # Increased number of values for C\n",
    "    'penalty': ['l1', 'l2'],       # Regularization type\n",
    "    'solver': ['liblinear', 'saga']  # Solvers that support l1 and l2 penalties\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
    "                                   n_iter=100, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "# Evaluate the model with the best parameters on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validation Score: {best_score:.4f}')\n",
    "print(f'Test Set Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with One-vs-One (Ovo) Multiclass Logistic Regression: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q10. Write a Python program to implement One-vs-One (Ovo) Multiclass Logistic Regression and print accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-10\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply One-vs-One (Ovo) Multiclass Logistic Regression\n",
    "ovo_model = OneVsOneClassifier(LogisticRegression(solver='liblinear'))\n",
    "ovo_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ovo_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy with One-vs-One (Ovo) Multiclass Logistic Regression: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.96\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAIjCAYAAACTRapjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAR81JREFUeJzt3Xt8z/X///H7e7OTnTdsUzbHhlBCjHJKSSqaHDow0qfSHEcHfVKorI9iDoXEBx0oJJ9KkrMIiSmpnGuVHUQ2c9hme/3+8PP+9jZq0957v7xft2uX1+Wy9/P1er+ej/e7i3l4PA8vm2EYhgAAAGAZHq4OAAAAAOWLBBAAAMBiSAABAAAshgQQAADAYkgAAQAALIYEEAAAwGJIAAEAACyGBBAAAMBiSAABAAAshgQQwF/at2+fbrvtNgUHB8tms2np0qVlev+ffvpJNptNc+fOLdP7Xsnatm2rtm3bujoMAG6MBBC4Ahw4cECPPvqoatasKV9fXwUFBalVq1aaPHmyTp8+7dS+ExIStGvXLr300kt6++231bRpU6f2V5769u0rm82moKCgi36P+/btk81mk81m06uvvlrq+x8+fFijR4/Wzp07yyBaACg7FVwdAIC/tmzZMnXv3l0+Pj7q06ePGjRooPz8fG3cuFFPPPGEdu/erZkzZzql79OnT2vz5s3697//rYEDBzqlj5iYGJ0+fVpeXl5Ouf/fqVChgk6dOqWPP/5YPXr0cDj37rvvytfXV2fOnLmsex8+fFhjxoxR9erVdf3115f4fZ9//vll9QcAJUUCCJjYoUOH1KtXL8XExGjNmjWKioqyn0tMTNT+/fu1bNkyp/V/5MgRSVJISIjT+rDZbPL19XXa/f+Oj4+PWrVqpQULFhRLAOfPn6/OnTvrgw8+KJdYTp06pYoVK8rb27tc+gNgXQwBAyY2fvx45ebmavbs2Q7J33m1a9fWkCFD7K/Pnj2rF154QbVq1ZKPj4+qV6+uZ555Rnl5eQ7vq169uu68805t3LhRN954o3x9fVWzZk299dZb9mtGjx6tmJgYSdITTzwhm82m6tWrSzo3dHr+5z8bPXq0bDabQ9vKlSt10003KSQkRAEBAYqNjdUzzzxjP3+pOYBr1qzRzTffLH9/f4WEhKhLly764YcfLtrf/v371bdvX4WEhCg4OFj9+vXTqVOnLv3FXuD+++/X8uXLdfz4cXvbtm3btG/fPt1///3Frj927JhGjBihhg0bKiAgQEFBQerUqZO++eYb+zXr1q1Ts2bNJEn9+vWzDyWf/5xt27ZVgwYNtH37drVu3VoVK1a0fy8XzgFMSEiQr69vsc/fsWNHhYaG6vDhwyX+rAAgkQACpvbxxx+rZs2aatmyZYmuf/jhh/Xcc8/phhtuUEpKitq0aaPk5GT16tWr2LX79+/Xvffeq1tvvVUTJkxQaGio+vbtq927d0uS4uPjlZKSIkm677779Pbbb2vSpEmlin/37t268847lZeXp7Fjx2rChAm6++67tWnTpr9836pVq9SxY0dlZWVp9OjRSkpK0pdffqlWrVrpp59+KnZ9jx49dOLECSUnJ6tHjx6aO3euxowZU+I44+PjZbPZtGTJEnvb/PnzVbduXd1www3Frj948KCWLl2qO++8UxMnTtQTTzyhXbt2qU2bNvZkrF69eho7dqwk6ZFHHtHbb7+tt99+W61bt7bf5+jRo+rUqZOuv/56TZo0Se3atbtofJMnT1blypWVkJCgwsJCSdIbb7yhzz//XFOnTlXVqlVL/FkBQJJkADCl7OxsQ5LRpUuXEl2/c+dOQ5Lx8MMPO7SPGDHCkGSsWbPG3hYTE2NIMjZs2GBvy8rKMnx8fIzhw4fb2w4dOmRIMl555RWHeyYkJBgxMTHFYnj++eeNP/9aSUlJMSQZR44cuWTc5/uYM2eOve366683qlSpYhw9etTe9s033xgeHh5Gnz59ivX30EMPOdzznnvuMcLDwy/Z558/h7+/v2EYhnHvvfcat9xyi2EYhlFYWGhERkYaY8aMueh3cObMGaOwsLDY5/Dx8THGjh1rb9u2bVuxz3ZemzZtDEnGjBkzLnquTZs2Dm0rVqwwJBkvvviicfDgQSMgIMDo2rXr335GALgYKoCASeXk5EiSAgMDS3T9p59+KklKSkpyaB8+fLgkFZsrWL9+fd18883215UrV1ZsbKwOHjx42TFf6Pzcwf/9738qKioq0XvS09O1c+dO9e3bV2FhYfb2Ro0a6dZbb7V/zj977LHHHF7ffPPNOnr0qP07LIn7779f69atU0ZGhtasWaOMjIyLDv9K5+YNenic+/VZWFioo0eP2oe3d+zYUeI+fXx81K9fvxJde9ttt+nRRx/V2LFjFR8fL19fX73xxhsl7gsA/owEEDCpoKAgSdKJEydKdP3PP/8sDw8P1a5d26E9MjJSISEh+vnnnx3ao6Oji90jNDRUf/zxx2VGXFzPnj3VqlUrPfzww4qIiFCvXr20cOHCv0wGz8cZGxtb7Fy9evX0+++/6+TJkw7tF36W0NBQSSrVZ7njjjsUGBio999/X++++66aNWtW7Ls8r6ioSCkpKapTp458fHxUqVIlVa5cWd9++62ys7NL3OdVV11VqgUfr776qsLCwrRz505NmTJFVapUKfF7AeDPSAABkwoKClLVqlX13Xfflep9Fy7CuBRPT8+LthuGcdl9nJ+fdp6fn582bNigVatWqXfv3vr222/Vs2dP3XrrrcWu/Sf+yWc5z8fHR/Hx8Zo3b54+/PDDS1b/JGncuHFKSkpS69at9c4772jFihVauXKlrr322hJXOqVz309ppKamKisrS5K0a9euUr0XAP6MBBAwsTvvvFMHDhzQ5s2b//bamJgYFRUVad++fQ7tmZmZOn78uH1Fb1kIDQ11WDF73oVVRkny8PDQLbfcookTJ+r777/XSy+9pDVr1mjt2rUXvff5OPfs2VPs3I8//qhKlSrJ39//n32AS7j//vuVmpqqEydOXHThzHmLFy9Wu3btNHv2bPXq1Uu33XabOnToUOw7KWkyXhInT55Uv379VL9+fT3yyCMaP368tm3bVmb3B2AtJICAiT355JPy9/fXww8/rMzMzGLnDxw4oMmTJ0s6N4QpqdhK3YkTJ0qSOnfuXGZx1apVS9nZ2fr222/tbenp6frwww8drjt27Fix957fEPnCrWnOi4qK0vXXX6958+Y5JFTfffedPv/8c/vndIZ27drphRde0GuvvabIyMhLXufp6Vmsurho0SL99ttvDm3nE9WLJcul9dRTTyktLU3z5s3TxIkTVb16dSUkJFzyewSAv8JG0ICJ1apVS/Pnz1fPnj1Vr149hyeBfPnll1q0aJH69u0rSbruuuuUkJCgmTNn6vjx42rTpo2++uorzZs3T127dr3kFiOXo1evXnrqqad0zz33aPDgwTp16pSmT5+ua665xmERxNixY7VhwwZ17txZMTExysrK0rRp03T11VfrpptuuuT9X3nlFXXq1ElxcXHq37+/Tp8+ralTpyo4OFijR48us89xIQ8PDz377LN/e92dd96psWPHql+/fmrZsqV27dqld999VzVr1nS4rlatWgoJCdGMGTMUGBgof39/NW/eXDVq1ChVXGvWrNG0adP0/PPP27elmTNnjtq2batRo0Zp/PjxpbofALANDHAF2Lt3r/Gvf/3LqF69uuHt7W0EBgYarVq1MqZOnWqcOXPGfl1BQYExZswYo0aNGoaXl5dRrVo1Y+TIkQ7XGMa5bWA6d+5crJ8Ltx+51DYwhmEYn3/+udGgQQPD29vbiI2NNd55551i28CsXr3a6NKli1G1alXD29vbqFq1qnHfffcZe/fuLdbHhVulrFq1ymjVqpXh5+dnBAUFGXfddZfx/fffO1xzvr8Lt5mZM2eOIck4dOjQJb9Tw3DcBuZSLrUNzPDhw42oqCjDz8/PaNWqlbF58+aLbt/yv//9z6hfv75RoUIFh8/Zpk0b49prr71on3++T05OjhETE2PccMMNRkFBgcN1w4YNMzw8PIzNmzf/5WcAgAvZDKMUs6QBAABwxWMOIAAAgMWQAAIAAFgMCSAAAIDFkAACAACYRPXq1WWz2YodiYmJkqQzZ84oMTFR4eHhCggIULdu3S66TdjfYREIAACASRw5csThSUnfffedbr31Vq1du1Zt27bVgAEDtGzZMs2dO1fBwcEaOHCgPDw8tGnTplL1QwIIAABgUkOHDtUnn3yiffv2KScnR5UrV9b8+fN17733Sjr3hKR69epp8+bNatGiRYnvyxAwAACAE+Xl5SknJ8fhKMlTfPLz8/XOO+/ooYceks1m0/bt21VQUKAOHTrYr6lbt66io6NL9MjQP3PLJ4E8+M43rg4BgJNMjW/g6hAAOEloRU+X9e3XeKDT7v1Ul0oaM2aMQ9vzzz//t082Wrp0qY4fP25/4lNGRoa8vb0VEhLicF1ERIQyMjJKFZNbJoAAAABmMXLkSCUlJTm0+fj4/O37Zs+erU6dOqlq1aplHhMJIAAAgM15s+J8fHxKlPD92c8//6xVq1ZpyZIl9rbIyEjl5+fr+PHjDlXAzMxMRUZGlur+zAEEAACw2Zx3XIY5c+aoSpUq6ty5s72tSZMm8vLy0urVq+1te/bsUVpamuLi4kp1fyqAAAAAJlJUVKQ5c+YoISFBFSr8X6oWHBys/v37KykpSWFhYQoKCtKgQYMUFxdXqhXAEgkgAACAU4eAS2vVqlVKS0vTQw89VOxcSkqKPDw81K1bN+Xl5aljx46aNm1aqftwy30AWQUMuC9WAQPuy6WrgJsOc9q9T3+d4rR7Xy4qgAAAAJc5V+9KZZ56JwAAAMoFFUAAAAATzQEsD9b6tAAAAKACCAAAYLU5gCSAAAAADAEDAADAnVEBBAAAsNgQMBVAAAAAi6ECCAAAwBxAAAAAuDMqgAAAAMwBBAAAgDujAggAAGCxOYAkgAAAAAwBAwAAwJ1RAQQAALDYELC1Pi0AAACoAAIAAFABBAAAgFujAggAAODBKmAAAAC4MSqAAAAAFpsDSAIIAADARtAAAABwZ1QAAQAALDYEbK1PCwAAACqAAAAAzAEEAACAW6MCCAAAwBxAAAAAuDMqgAAAABabA0gCCAAAwBAwAAAA3BkVQAAAAIsNAVMBBAAAsBgqgAAAAMwBBAAAgDujAggAAMAcQAAAALgzKoAAAAAWmwNIAggAAGCxBNBanxYAAABUAAEAAFgEAgAAALdGBRAAAIA5gAAAAHBnVAABAACYAwgAAAB3RgUQAADAYnMASQABAAAYAgYAAIA7owIIAAAsz0YFEAAAAO6MCiAAALA8KoAAAABwaySAAAAANicepfTbb7/pwQcfVHh4uPz8/NSwYUN9/fXX9vOGYei5555TVFSU/Pz81KFDB+3bt69UfZAAAgAAmMQff/yhVq1aycvLS8uXL9f333+vCRMmKDQ01H7N+PHjNWXKFM2YMUNbt26Vv7+/OnbsqDNnzpS4H+YAAgAAyzPLHMD//Oc/qlatmubMmWNvq1Gjhv1nwzA0adIkPfvss+rSpYsk6a233lJERISWLl2qXr16lagfKoAAAMDybDab0468vDzl5OQ4HHl5eReN46OPPlLTpk3VvXt3ValSRY0bN9abb75pP3/o0CFlZGSoQ4cO9rbg4GA1b95cmzdvLvHnJQEEAABwouTkZAUHBzscycnJF7324MGDmj59uurUqaMVK1ZowIABGjx4sObNmydJysjIkCRFREQ4vC8iIsJ+riQYAgYAAJbnzCHgkSNHKikpyaHNx8fnotcWFRWpadOmGjdunCSpcePG+u677zRjxgwlJCSUWUxUAAEAAJzIx8dHQUFBDselEsCoqCjVr1/foa1evXpKS0uTJEVGRkqSMjMzHa7JzMy0nysJEkAAAGB5zpwDWBqtWrXSnj17HNr27t2rmJgYSecWhERGRmr16tX28zk5Odq6davi4uJK3A9DwAAAACYxbNgwtWzZUuPGjVOPHj301VdfaebMmZo5c6akc4nq0KFD9eKLL6pOnTqqUaOGRo0apapVq6pr164l7ocEEAAAwBy7wKhZs2b68MMPNXLkSI0dO1Y1atTQpEmT9MADD9ivefLJJ3Xy5Ek98sgjOn78uG666SZ99tln8vX1LXE/NsMwDGd8AFd68J1vXB0CACeZGt/A1SEAcJLQip4u6zv4/reddu/s+b2ddu/LRQUQAABYnlk2gi4vLAIBAACwGCqAAADA8qxWASQBBAAAlme1BJAhYAAAAIuhAggAACyPCiAAAADcGhVAAAAAaxUAqQACAABYDRVAAABgecwBdAFPT09lZWUVaz969Kg8PV33WBgAAAB3ZIoK4KUeR5yXlydvb+9yjgYAAFiN1SqALk0Ap0yZIunclz5r1iwFBATYzxUWFmrDhg2qW7euq8IDAAAWQQJYjlJSUiSdqwDOmDHDYbjX29tb1atX14wZM1wVHgAAgFtyaQJ46NAhSVK7du20ZMkShYaGujIcAABgVdYqAJpjDuDatWtdHQIAAIBlmCIBLCws1Ny5c7V69WplZWWpqKjI4fyaNWtcFBkAALAC5gC6wJAhQzR37lx17txZDRo0sNz/BAAAgPJkigTwvffe08KFC3XHHXe4OhQAAGBBVis+mWIjaG9vb9WuXdvVYQAAAFiCKRLA4cOHa/LkyZfcEBoAAMCZbDab0w4zMsUQ8MaNG7V27VotX75c1157rby8vBzOL1myxEWRAQAAKzBrouYspkgAQ0JCdM8997g6DAAAAEswRQI4Z84cV4cAAACszFoFQHPMAQQAAED5MUUFUJIWL16shQsXKi0tTfn5+Q7nduzY4aKoAACAFVhtDqApKoBTpkxRv379FBERodTUVN14440KDw/XwYMH1alTJ1eHBwAA4FZMkQBOmzZNM2fO1NSpU+Xt7a0nn3xSK1eu1ODBg5Wdne3q8AAAgJuz2jYwpkgA09LS1LJlS0mSn5+fTpw4IUnq3bu3FixY4MrQAAAA3I4pEsDIyEgdO3ZMkhQdHa0tW7ZIkg4dOsTm0AAAwOmoALpA+/bt9dFHH0mS+vXrp2HDhunWW29Vz5492R8QAAA4n82JhwmZYhXwzJkzVVRUJElKTExUeHi4vvzyS91999169NFHXRwdAACAezFFAujh4SEPj/8rRvbq1Uu9evVyYUQAAMBKzDpU6yymSAAl6fjx4/rqq6+UlZVlrwae16dPHxdFBQAA4H5MkQB+/PHHeuCBB5Sbm6ugoCCHLNxms5EAAgAAp7JaBdAUi0CGDx+uhx56SLm5uTp+/Lj++OMP+3F+dTAAAADKhikqgL/99psGDx6sihUrujoUmNQtdcJ1yzXhquzvLUn6NfuMPtyVqW8Pn9szskqAt+6/oaquqeIvLw+bvk0/oXnbflPOmbOuDBtAGXjrv29q2tQU9by/t4Y9MdLV4cBNUQF0gY4dO+rrr792dRgwsWOnCvR+arqeXb5Xo5bv1fcZuUpqU11XBfvIx9NDT91SU4YMjVt1QGM+3y9PD5uGt61h1tX3AEro+9279OEHC1W7TqyrQwHciikqgJ07d9YTTzyh77//Xg0bNpSXl5fD+bvvvttFkcEsUn/LcXi96JsM3XJNuGpX8ldoxXxV9vfWs5/u1emCcwuI3vgyTW/0aKD6kQHanZHripAB/EOnTp3U8888qZGjxmjOrDdcHQ7cnNUqgKZIAP/1r39JksaOHVvsnM1mU2FhYXmHBBOz2aTm0SHyqeChfb+fVESAjwxJBYX/99SYgkJDhiHFVvEnAQSuUK8mv6hWN7fRjS1akgDC+ayV/5kjAbxw25fSyMvLU15enkNbYUG+PL28/2lYMJmrQ3w1umNteXl66MzZIk1a/5MOZ+fpxJmzyjtbpF6No7RwZ7pssqln4yh5etgU4uf19zcGYDorP/tUe378Xv99Z6GrQwHckinmAP4TycnJCg4Odjh2fzzb1WHBCdJz8vTvZXv1/Gf7tHrv73q0ZbSqBvvoRF6hpnzxkxpfHaRZvRpqZs8GqujtoUNHT6mIZ0kDV5zMjHRNfCVZo18aLx8fH1eHA4uw2rOAbYbh+r8hp0yZctF2m80mX19f1a5dW61bt5anp2exay5WAXz0gz1UAC3g6VtqKis3X//d+qu9LcDHU0VFhk4VFOm1bvW1/IcjWvb9ERdGibI2Nb6Bq0OAk61fu0pPJQ12+J1fWFgom80mDw8Pbdi686J/H+DKF1rRdf9fayZ96rR7H5x4h9PufblMMQSckpKiI0eO6NSpUwoNDZUk/fHHH6pYsaICAgKUlZWlmjVrau3atapWrZrDe318fIr9C5HkzxpsNqmCh+O/rHLzzs0XrR8RoCDfCtrxa87F3grAxJreGKd3F/3Poe3F5/+tmBo11LvvwyR/cAqzVuqcxRRDwOPGjVOzZs20b98+HT16VEePHtXevXvVvHlzTZ48WWlpaYqMjNSwYcNcHSpcpMf1kYqt4q9K/l66OsRXPa6PVL2IAH156A9JUuuaoapVqaKqBHirVY0QDWodo89+OKL0nLy/uTMAs/H391et2nUcDl8/PwUHh6hW7TquDg9wC6aoAD777LP64IMPVKtWLXtb7dq19eqrr6pbt246ePCgxo8fr27durkwSrhSkG8FPdYyWiF+FXSqoFC//HFG41cf1Hf/f4VvVJCvejSOUoC3p46cLNBH32Vq+Q+/uzhqAMCVwmIFQHMkgOnp6Tp7tvgTG86ePauMjAxJUtWqVXXixInyDg0mMWvLr395/v2d6Xp/Z3o5RQOgvE2fNc/VIQBuxRRDwO3atdOjjz6q1NRUe1tqaqoGDBig9u3bS5J27dqlGjVquCpEAADgxqy2CtgUCeDs2bMVFhamJk2a2Bd1NG3aVGFhYZo9+9yWLgEBAZowYYKLIwUAAO7IZnPeYUamGAKOjIzUypUr9eOPP2rv3r2SpNjYWMXG/t+zH9u1a+eq8AAAANyKKRLA8+rWrau6deu6OgwAAGAxZh2qdRaXJYBJSUl64YUX5O/vr6SkpL+8duLEieUUFQAAgPtzWQKYmpqqgoIC+8+XYrWMHAAAlD+rpRsuSwDXrl170Z8BAADgXKaaAwgAAOAKHh7WKgG6LAGMj48v8bVLlixxYiQAAADW4rJ9AIODg0t8AAAAOJNZ9gEcPXp0sY2k/7xDypkzZ5SYmKjw8HAFBASoW7duyszMLPXndVkFcM6cOa7qGgAAwIGZFp1ee+21WrVqlf11hQr/l64NGzZMy5Yt06JFixQcHKyBAwcqPj5emzZtKlUfzAEEAAAwkQoVKigyMrJYe3Z2tmbPnq358+fbH5U7Z84c1atXT1u2bFGLFi1K3keZRfsPLV68WAsXLlRaWpry8/Mdzu3YscNFUQEAACtwZgEwLy9PeXl5Dm3nH317Mfv27VPVqlXl6+uruLg4JScnKzo6Wtu3b1dBQYE6dOhgv7Zu3bqKjo7W5s2bS5UAmuJZwFOmTFG/fv0UERGh1NRU3XjjjQoPD9fBgwfVqVMnV4cHAABw2ZKTk4utb0hOTr7otc2bN9fcuXP12Wefafr06Tp06JBuvvlmnThxQhkZGfL29lZISIjDeyIiIpSRkVGqmExRAZw2bZpmzpyp++67T3PnztWTTz6pmjVr6rnnntOxY8dcHR4AAHBzzpwDOHLkyGJPPbtU9e/Pha9GjRqpefPmiomJ0cKFC+Xn51dmMZmiApiWlqaWLVtKkvz8/HTixAlJUu/evbVgwQJXhgYAAPCP+Pj4KCgoyOG4VAJ4oZCQEF1zzTXav3+/IiMjlZ+fr+PHjztck5mZedE5g3/FFAlgZGSkvdIXHR2tLVu2SJIOHTokwzBcGRoAALCAC7deKcvjn8jNzdWBAwcUFRWlJk2ayMvLS6tXr7af37Nnj9LS0hQXF1eq+5piCLh9+/b66KOP1LhxY/Xr10/Dhg3T4sWL9fXXX5dqw2gAAIAr2YgRI3TXXXcpJiZGhw8f1vPPPy9PT0/dd999Cg4OVv/+/ZWUlKSwsDAFBQVp0KBBiouLK9UCEMkkCeDMmTNVVFQkSUpMTFSlSpW0adMm3X333XrsscdcHB0AAHB3ZtkG8Ndff9V9992no0ePqnLlyrrpppu0ZcsWVa5cWZKUkpIiDw8PdevWTXl5eerYsaOmTZtW6n5shknGWM+cOaNvv/1WWVlZ9mRQOleSveuuu0p1rwff+aaswwNgElPjG7g6BABOElrR02V9Nx6zxmn3Tn2+vdPufblMUQH87LPP1Lt3bx09erTYOZvNpsLCQhdEBQAA4J5MsQhk0KBB6tGjh9LT01VUVORwkPwBAABnM8uzgMuLKRLAzMxMJSUlKSIiwtWhAAAAuD1TJID33nuv1q1b5+owAACARZl1GxhnMcUcwNdee03du3fXF198oYYNG8rLy8vh/ODBg10UGQAAgPsxRQK4YMECff755/L19dW6descsmWbzUYCCAAAnMqkhTqnMUUC+O9//1tjxozR008/LQ8PU4xKAwAAuC1TJID5+fnq2bMnyR8AAHAJs87VcxZTZFwJCQl6//33XR0GAACAJZiiAlhYWKjx48drxYoVatSoUbFFIBMnTnRRZAAAwAosVgA0RwK4a9cuNW7cWJL03XffOZyzWkkWAACUP6vlG6ZIANeuXevqEAAAACzDFAkgAACAK1msAGiORSAAAAAoP1QAAQCA5VltDiAVQAAAAIuhAggAACzPYgVAKoAAAABWQwUQAABYntXmAJIAAgAAy7NY/scQMAAAgNVQAQQAAJZntSFgKoAAAAAWQwUQAABYHhVAAAAAuDUqgAAAwPIsVgCkAggAAGA1VAABAIDlWW0OIAkgAACwPIvlfwwBAwAAWA0VQAAAYHlWGwKmAggAAGAxVAABAIDlWawASAUQAADAaqgAAgAAy/OwWAmQCiAAAIDFUAEEAACWZ7ECIAkgAAAA28AAAADArVEBBAAAludhrQIgFUAAAACroQIIAAAsjzmAAAAAcGtUAAEAgOVZrABIBRAAAMBqqAACAADLs8laJUASQAAAYHlsAwMAAAC3RgUQAABYHtvAAAAAwK1RAQQAAJZnsQIgFUAAAACroQIIAAAsz8NiJUAqgAAAABZDBRAAAFiexQqAJIAAAABsAwMAAABTePnll2Wz2TR06FB725kzZ5SYmKjw8HAFBASoW7duyszMLNV9SQABAIDl2WzOOy7Xtm3b9MYbb6hRo0YO7cOGDdPHH3+sRYsWaf369Tp8+LDi4+NLdW8SQAAAAJPJzc3VAw88oDfffFOhoaH29uzsbM2ePVsTJ05U+/bt1aRJE82ZM0dffvmltmzZUuL7kwACAADL87DZnHbk5eUpJyfH4cjLy/vLeBITE9W5c2d16NDBoX379u0qKChwaK9bt66io6O1efPmkn/e0n09AAAAKI3k5GQFBwc7HMnJyZe8/r333tOOHTsuek1GRoa8vb0VEhLi0B4REaGMjIwSx8QqYAAAYHnOXAM8cuRIJSUlObT5+Phc9NpffvlFQ4YM0cqVK+Xr6+u0mEgAAQAAnMjHx+eSCd+Ftm/frqysLN1www32tsLCQm3YsEGvvfaaVqxYofz8fB0/ftyhCpiZmanIyMgSx0QCCAAALM8s+wDecsst2rVrl0Nbv379VLduXT311FOqVq2avLy8tHr1anXr1k2StGfPHqWlpSkuLq7E/ZAAAgAAy/MwR/6nwMBANWjQwKHN399f4eHh9vb+/fsrKSlJYWFhCgoK0qBBgxQXF6cWLVqUuB8SQAAAgCtISkqKPDw81K1bN+Xl5aljx46aNm1aqe5BAggAACzPLEPAF7Nu3TqH176+vnr99df1+uuvX/Y92QYGAADAYqgAAgAAyzNxAdApqAACAABYDBVAAABgeWaeA+gMJUoAP/rooxLf8O67777sYAAAAOB8JUoAu3btWqKb2Ww2FRYW/pN4AAAAyp1Z9gEsLyVKAIuKipwdBwAAgMtYbQiYRSAAAAAWc1mLQE6ePKn169crLS1N+fn5DucGDx5cJoEBAACUF2vV/y4jAUxNTdUdd9yhU6dO6eTJkwoLC9Pvv/+uihUrqkqVKiSAAAAAJlfqIeBhw4bprrvu0h9//CE/Pz9t2bJFP//8s5o0aaJXX33VGTECAAA4lYfN5rTDjEqdAO7cuVPDhw+Xh4eHPD09lZeXp2rVqmn8+PF65plnnBEjAAAAylCpE0AvLy95eJx7W5UqVZSWliZJCg4O1i+//FK20QEAAJQDm815hxmVeg5g48aNtW3bNtWpU0dt2rTRc889p99//11vv/22GjRo4IwYAQAAUIZKXQEcN26coqKiJEkvvfSSQkNDNWDAAB05ckQzZ84s8wABAACczWazOe0wo1JXAJs2bWr/uUqVKvrss8/KNCAAAAA412XtAwgAAOBOTFqoc5pSJ4A1atT4y3LmwYMH/1FAAAAA5c2s27U4S6kTwKFDhzq8LigoUGpqqj777DM98cQTZRUXAAAAnKTUCeCQIUMu2v7666/r66+//scBAQAAlDeLFQBLvwr4Ujp16qQPPvigrG4HAAAAJymzRSCLFy9WWFhYWd0OAACg3Jh1uxZnuayNoP/8JRmGoYyMDB05ckTTpk0r0+AAAABQ9kqdAHbp0sUhAfTw8FDlypXVtm1b1a1bt0yDu1yzel3n6hAAOElos4GuDgGAk5xOfc1lfZfZnLgrRKkTwNGjRzshDAAAAJSXUie8np6eysrKKtZ+9OhReXp6lklQAAAA5YlHwf0NwzAu2p6Xlydvb+9/HBAAAEB58zBnnuY0JU4Ap0yZIulchjxr1iwFBATYzxUWFmrDhg2mmQMIAACASytxApiSkiLpXAVwxowZDsO93t7eql69umbMmFH2EQIAADgZFcBLOHTokCSpXbt2WrJkiUJDQ50WFAAAAJyn1HMA165d64w4AAAAXMasizWcpdSrgLt166b//Oc/xdrHjx+v7t27l0lQAAAAcJ5SJ4AbNmzQHXfcUay9U6dO2rBhQ5kEBQAAUJ48bM47zKjUCWBubu5Ft3vx8vJSTk5OmQQFAAAA5yl1AtiwYUO9//77xdrfe+891a9fv0yCAgAAKE82m/MOMyr1IpBRo0YpPj5eBw4cUPv27SVJq1ev1vz587V48eIyDxAAAMDZPMyaqTlJqRPAu+66S0uXLtW4ceO0ePFi+fn56brrrtOaNWsUFhbmjBgBAABQhkqdAEpS586d1blzZ0lSTk6OFixYoBEjRmj79u0qLCws0wABAACcrdRz4q5wl/15N2zYoISEBFWtWlUTJkxQ+/bttWXLlrKMDQAAAE5QqgpgRkaG5s6dq9mzZysnJ0c9evRQXl6eli5dygIQAABwxbLYFMCSVwDvuusuxcbG6ttvv9WkSZN0+PBhTZ061ZmxAQAAwAlKXAFcvny5Bg8erAEDBqhOnTrOjAkAAKBcWW0VcIkrgBs3btSJEyfUpEkTNW/eXK+99pp+//13Z8YGAAAAJyhxAtiiRQu9+eabSk9P16OPPqr33ntPVatWVVFRkVauXKkTJ044M04AAACnsdpG0KVeBezv76+HHnpIGzdu1K5duzR8+HC9/PLLqlKliu6++25nxAgAAOBUPAu4FGJjYzV+/Hj9+uuvWrBgQVnFBAAAACe6rI2gL+Tp6amuXbuqa9euZXE7AACAcsUiEAAAALi1MqkAAgAAXMksVgCkAggAAGA1VAABAIDlmXW1rrNQAQQAALAYKoAAAMDybLJWCZAEEAAAWB5DwAAAAHBrVAABAIDlUQEEAACAS0yfPl2NGjVSUFCQgoKCFBcXp+XLl9vPnzlzRomJiQoPD1dAQIC6deumzMzMUvdDAggAACzPZrM57SiNq6++Wi+//LK2b9+ur7/+Wu3bt1eXLl20e/duSdKwYcP08ccfa9GiRVq/fr0OHz6s+Pj40n9ewzCMUr/L5M6cdXUEAJwltNlAV4cAwElOp77msr5fWXfQafd+om3Nf/T+sLAwvfLKK7r33ntVuXJlzZ8/X/fee68k6ccff1S9evW0efNmtWjRosT3ZA4gAACwPGfOAczLy1NeXp5Dm4+Pj3x8fP7yfYWFhVq0aJFOnjypuLg4bd++XQUFBerQoYP9mrp16yo6OrrUCSBDwAAAAE6UnJys4OBghyM5OfmS1+/atUsBAQHy8fHRY489pg8//FD169dXRkaGvL29FRIS4nB9RESEMjIyShUTFUAAAGB5pZyqVyojR45UUlKSQ9tfVf9iY2O1c+dOZWdna/HixUpISND69evLNCYSQAAAYHkeTswASzLc+2fe3t6qXbu2JKlJkybatm2bJk+erJ49eyo/P1/Hjx93qAJmZmYqMjKyVDExBAwAAGBiRUVFysvLU5MmTeTl5aXVq1fbz+3Zs0dpaWmKi4sr1T2pAAIAAMszy0bQI0eOVKdOnRQdHa0TJ05o/vz5WrdunVasWKHg4GD1799fSUlJCgsLU1BQkAYNGqS4uLhSLQCRSAABAABMIysrS3369FF6erqCg4PVqFEjrVixQrfeeqskKSUlRR4eHurWrZvy8vLUsWNHTZs2rdT9sA8ggCsK+wAC7suV+wBO3XTIafce1KqG0+59uZgDCAAAYDEMAQMAAMvzkEkmAZYTKoAAAAAWQwUQAABYnjM3gjYjEkAAAGB5ZtkGprwwBAwAAGAxVAABAIDlOfNRcGZEBRAAAMBiqAACAADLs1gBkAogAACA1VABBAAAlsccQAAAALg1KoAAAMDyLFYAJAEEAACw2pCo1T4vAACA5VEBBAAAlmez2BgwFUAAAACLoQIIAAAsz1r1PyqAAAAAlkMFEAAAWB4bQQMAAMCtUQEEAACWZ636HwkgAACA5Z4EwhAwAACAxVABBAAAlsdG0AAAAHBrVAABAIDlWa0iZrXPCwAAYHlUAAEAgOUxBxAAAABujQogAACwPGvV/6gAAgAAWA4VQAAAYHlWmwNIAggAACzPakOiVvu8AAAAlkcFEAAAWJ7VhoCpAAIAAFgMFUAAAGB51qr/UQEEAACwHCqAAADA8iw2BZAKIAAAgNVQAQQAAJbnYbFZgCSAAADA8hgCBgAAgFujAggAACzPZrEhYCqAAAAAFkMFEAAAWB5zAAEAAODWqAACAADLs9o2MFQAAQAALIYKIAAAsDyrzQEkAQQAAJZHAugi+/bt09q1a5WVlaWioiKHc88995yLogIAAHA/pkgA33zzTQ0YMECVKlVSZGSkbH9Kw202GwkgAABwKqttBG2KBPDFF1/USy+9pKeeesrVoQAAALg9UySAf/zxh7p37+7qMAAAgEV5WKsAaI5tYLp3767PP//c1WEAAAC4VHJyspo1a6bAwEBVqVJFXbt21Z49exyuOXPmjBITExUeHq6AgAB169ZNmZmZperHFBXA2rVra9SoUdqyZYsaNmwoLy8vh/ODBw92UWQAAMAKzDIHcP369UpMTFSzZs109uxZPfPMM7rtttv0/fffy9/fX5I0bNgwLVu2TIsWLVJwcLAGDhyo+Ph4bdq0qcT92AzDMJz1IUqqRo0alzxns9l08ODBUt3vzNl/GhEAswptNtDVIQBwktOpr7ms7zU/HnXavdvXDb/s9x45ckRVqlTR+vXr1bp1a2VnZ6ty5cqaP3++7r33XknSjz/+qHr16mnz5s1q0aJFie5rigrgoUOHXB0CAACwMGfuA5iXl6e8vDyHNh8fH/n4+Pzte7OzsyVJYWFhkqTt27eroKBAHTp0sF9Tt25dRUdHlyoBNMUcQAAAAFeyOfG/5ORkBQcHOxzJycl/G1NRUZGGDh2qVq1aqUGDBpKkjIwMeXt7KyQkxOHaiIgIZWRklPjzmqICmJSUdNF2m80mX19f1a5dW126dLFnvwAAAFeKkSNHFst1SlL9S0xM1HfffaeNGzeWeUymSABTU1O1Y8cOFRYWKjY2VpK0d+9eeXp6qm7dupo2bZqGDx+ujRs3qn79+i6OFgAAuBtnbgNT0uHePxs4cKA++eQTbdiwQVdffbW9PTIyUvn5+Tp+/LhDFTAzM1ORkZElvr8phoC7dOmiDh066PDhw9q+fbu2b9+uX3/9Vbfeeqvuu+8+/fbbb2rdurWGDRvm6lABAACcxjAMDRw4UB9++KHWrFlTbKFskyZN5OXlpdWrV9vb9uzZo7S0NMXFxZW4H1OsAr7qqqu0cuXKYtW93bt367bbbtNvv/2mHTt26LbbbtPvv//+t/djFTDgvlgFDLgvV64C/mLvH067983XhJb42scff1zz58/X//73P/uoqCQFBwfLz89PkjRgwAB9+umnmjt3roKCgjRo0CBJ0pdfflnifkxRAczOzlZWVlax9iNHjignJ0eSFBISovz8/PIODQAAoNxMnz5d2dnZatu2raKiouzH+++/b78mJSVFd955p7p166bWrVsrMjJSS5YsKVU/ppgD2KVLFz300EOaMGGCmjVrJknatm2bRowYoa5du0qSvvrqK11zzTUujBJmsv3rbZr739n64fvvdOTIEaVMeV3tb+nw928EYDo/LhujmKrF90mb8f4GDXt5oXy8K+jlpHh179hEPt4VtGrzDxoy7n1lHTvhgmjhrpy5DUxplGRg1tfXV6+//rpef/31y+7HFAngG2+8oWHDhqlXr146e/bc+G2FChWUkJCglJQUSef2uJk1a5Yrw4SJnD59SrGxseoa301JQxgSBK5kNz34ijz/NAO/fu2q+nTGIC1ZmSpJGj+imzrddK0eeHK2cnJPK+XpHnpvwsNq3y/FVSEDVzxTJIABAQF68803lZKSYn/qR82aNRUQEGC/5vrrr3dRdDCjm25uo5tubuPqMACUgd//yHV4PaJfAx1IO6Ivtu9TUICv+naNU99n5mr9tr2SpEeef0fffDhKNzasrq92/eSCiOGOTFIALDemSADPCwgIUKNGjVwdBgDARbwqeKrXHc005Z01kqTG9aLl7VVBa7bssV+z96dMpaUfU/NGNUgAUWY8zDIGXE5clgDGx8fbV6/Ex8f/5bV/NbHxYo9XMTxLv98OAMD17m7XSCGBfnrn462SpMjwIOXlFyg797TDdVlHcxQRHuSKEAG34LJVwMHBwbL9/2z7wsejXHj8lYs9XuWV//z941UAAOaT0LWlVmz6XulHsl0dCizG5sTDjFxWAZwzZ85Ffy6tiz1exfCk+gcAV5roqFC1bx6rXiPetLdlHM2Rj7eXggP8HKqAVcKDlHk0xxVhAm7BFPsA/hM+Pj4KCgpyOBj+BYArT++745R17ISWf7Hb3pb6Q5ryC86qXfP/2xC3TkwVRUeFaeu3h1wRJtyVxUqAplgEkpmZqREjRmj16tXKysoqtgdOYWGhiyKDWZ06eVJpaWn217/9+qt+/OEHBQcHK6pqVRdGBuBy2Gw29enSQu9+slWFhUX29pzcM5q7dLP+Mzxex7JP6sTJM5r4VHdt+eYgC0CAf8AUCWDfvn2VlpamUaNGKSoqyj43ELiU3bu/08P9+thfvzr+3LzPu7vcoxfGveyqsABcpvbNYxUdFaZ5S7cUO/fkqx+oqMjQglcfPrcR9Jc/aEjy+xe5C3D5bGYt1TmJKZ4FHBgYqC+++KLM9vrjWcCA++JZwID7cuWzgLcecN7Co+a1/npBqyuYogJYrVq1Ej36BAAAwBmsNvhoikUgkyZN0tNPP62ffvrJ1aEAAAALstgaEHNUAHv27KlTp06pVq1aqlixory8vBzOHzt2zEWRAQAAuB9TJICTJk1ydQgAAMDKzFqqcxJTJIAJCQmuDgEAAMAyTDEHUJIOHDigZ599Vvfdd5+ysrIkScuXL9fu3bv/5p0AAAD/jM2J/5mRKRLA9evXq2HDhtq6dauWLFmi3NxcSdI333yj559/3sXRAQAAuBdTJIBPP/20XnzxRa1cuVLe3t729vbt22vLluKbggIAAJQlm815hxmZIgHctWuX7rnnnmLtVapU0e+//+6CiAAAANyXKRLAkJAQpaenF2tPTU3VVVdd5YKIAACAlVhtH0BTJIC9evXSU089pYyMDNlsNhUVFWnTpk0aMWKE+vTp8/c3AAAA+CcslgGaIgEcN26c6tatq2rVqik3N1f169fXzTffrJYtW+rZZ591dXgAAABuxWaY6CG8v/zyi3bt2qWTJ0+qcePGql279mXd58zZMg4MgGmENhvo6hAAOMnp1Ndc1nfqzyecdu/GMYFOu/flMsVG0JI0e/ZspaSkaN++fZKkOnXqaOjQoXr44YddHBkAAIB7MUUC+Nxzz2nixIkaNGiQ4uLiJEmbN2/WsGHDlJaWprFjx7o4QgAA4M7Mul2Ls5hiCLhy5cqaMmWK7rvvPof2BQsWaNCgQaXeCoYhYMB9MQQMuC9XDgHvTHPeEPD10QwBX1RBQYGaNm1arL1JkyY6e5ZsDgAAOJfFCoDmWAXcu3dvTZ8+vVj7zJkz9cADD7ggIgAAAPflsgpgUlKS/WebzaZZs2bp888/V4sWLSRJW7duVVpaGvsAAgAA57NYCdBlCWBqaqrD6yZNmkiSDhw4IEmqVKmSKlWqpN27d5d7bAAAwFpsFssAXZYArl271lVdAwAAWJopFoEAAAC4ktW2gTHFIhAAAACUHyqAAADA8ixWAKQCCAAAYDVUAAEAACxWAqQCCAAAYDFUAAEAgOVZbR9AKoAAAAAWQwUQAABYntX2ASQBBAAAlmex/I8hYAAAAKuhAggAAGCxEiAVQAAAAIuhAggAACyPbWAAAADg1qgAAgAAy7PaNjBUAAEAACyGCiAAALA8ixUASQABAACslgEyBAwAAGAxVAABAIDlsQ0MAAAA3BoVQAAAYHlsAwMAAAC3RgUQAABYnsUKgFQAAQAAzGTDhg266667VLVqVdlsNi1dutThvGEYeu655xQVFSU/Pz916NBB+/btK1UfJIAAAAA2Jx6ldPLkSV133XV6/fXXL3p+/PjxmjJlimbMmKGtW7fK399fHTt21JkzZ0rcB0PAAADA8py5DUxeXp7y8vIc2nx8fOTj43PR6zt16qROnTpd9JxhGJo0aZKeffZZdenSRZL01ltvKSIiQkuXLlWvXr1KFBMVQAAAACdKTk5WcHCww5GcnHxZ9zp06JAyMjLUoUMHe1twcLCaN2+uzZs3l/g+VAABAIDlOXMbmJEjRyopKcmh7VLVv7+TkZEhSYqIiHBoj4iIsJ8rCRJAAAAAJ/qr4V5XYQgYAABYnonWgPylyMhISVJmZqZDe2Zmpv1cSZAAAgAAXCFq1KihyMhIrV692t6Wk5OjrVu3Ki4ursT3YQgYAADARDtB5+bmav/+/fbXhw4d0s6dOxUWFqbo6GgNHTpUL774ourUqaMaNWpo1KhRqlq1qrp27VriPkgAAQAATOTrr79Wu3bt7K/PLyBJSEjQ3Llz9eSTT+rkyZN65JFHdPz4cd1000367LPP5OvrW+I+bIZhGGUeuYudOevqCAA4S2izga4OAYCTnE59zWV9/3w07+8vukwx4eZaACJRAQQAAHDqNjBmxCIQAAAAi6ECCAAALM9iBUAqgAAAAFZDBRAAAFgecwABAADg1qgAAgAAWGwWIBVAAAAAi6ECCAAALM9qcwBJAAEAgOVZLP9jCBgAAMBqqAACAADLs9oQMBVAAAAAi6ECCAAALM9msVmAVAABAAAshgogAACAtQqAVAABAACshgogAACwPIsVAEkAAQAA2AYGAAAAbo0KIAAAsDy2gQEAAIBbowIIAABgrQIgFUAAAACroQIIAAAsz2IFQCqAAAAAVkMFEAAAWJ7V9gEkAQQAAJbHNjAAAABwa1QAAQCA5VltCJgKIAAAgMWQAAIAAFgMCSAAAIDFMAcQAABYHnMAAQAA4NaoAAIAAMuz2j6AJIAAAMDyGAIGAACAW6MCCAAALM9iBUAqgAAAAFZDBRAAAMBiJUAqgAAAABZDBRAAAFie1baBoQIIAABgMVQAAQCA5bEPIAAAANwaFUAAAGB5FisAkgACAABYLQNkCBgAAMBiqAACAADLYxsYAAAAuDUqgAAAwPLYBgYAAABuzWYYhuHqIIDLlZeXp+TkZI0cOVI+Pj6uDgdAGeLPN+A8JIC4ouXk5Cg4OFjZ2dkKCgpydTgAyhB/vgHnYQgYAADAYkgAAQAALIYEEAAAwGJIAHFF8/Hx0fPPP88EccAN8ecbcB4WgQAAAFgMFUAAAACLIQEEAACwGBJAAAAAiyEBhKn07dtXXbt2tb9u27athg4d6rJ4AJRMefxZvfD3A4DLV8HVAQB/ZcmSJfLy8nJ1GBdVvXp1DR06lAQVKCeTJ08W6xaBskECCFMLCwtzdQgATCI4ONjVIQBugyFgXLa2bdtq0KBBGjp0qEJDQxUREaE333xTJ0+eVL9+/RQYGKjatWtr+fLlkqTCwkL1799fNWrUkJ+fn2JjYzV58uS/7ePPFbb09HR17txZfn5+qlGjhubPn6/q1atr0qRJ9mtsNptmzZqle+65RxUrVlSdOnX00Ucf2c+XJI7zQ02vvvqqoqKiFB4ersTERBUUFNjj+vnnnzVs2DDZbDbZbLZ/+G0CV76zZ89q4MCBCg4OVqVKlTRq1Ch7xS4vL08jRozQVVddJX9/fzVv3lzr1q2zv3fu3LkKCQnRihUrVK9ePQUEBOj2229Xenq6/ZoLh4BPnDihBx54QP7+/oqKilJKSkqx3xnVq1fXuHHj9NBDDykwMFDR0dGaOXOms78KwPRIAPGPzJs3T5UqVdJXX32lQYMGacCAAerevbtatmypHTt26LbbblPv3r116tQpFRUV6eqrr9aiRYv0/fff67nnntMzzzyjhQsXlri/Pn366PDhw1q3bp0++OADzZw5U1lZWcWuGzNmjHr06KFvv/1Wd9xxhx544AEdO3ZMkkocx9q1a3XgwAGtXbtW8+bN09y5czV37lxJ54amr776ao0dO1bp6ekOf0kBVjVv3jxVqFBBX331lSZPnqyJEydq1qxZkqSBAwdq8+bNeu+99/Ttt9+qe/fuuv3227Vv3z77+0+dOqVXX31Vb7/9tjZs2KC0tDSNGDHikv0lJSVp06ZN+uijj7Ry5Up98cUX2rFjR7HrJkyYoKZNmyo1NVWPP/64BgwYoD179pT9FwBcSQzgMrVp08a46aab7K/Pnj1r+Pv7G71797a3paenG5KMzZs3X/QeiYmJRrdu3eyvExISjC5dujj0MWTIEMMwDOOHH34wJBnbtm2zn9+3b58hyUhJSbG3STKeffZZ++vc3FxDkrF8+fJLfpaLxRETE2OcPXvW3ta9e3ejZ8+e9tcxMTEO/QJW1qZNG6NevXpGUVGRve2pp54y6tWrZ/z888+Gp6en8dtvvzm855ZbbjFGjhxpGIZhzJkzx5Bk7N+/337+9ddfNyIiIuyv//z7IScnx/Dy8jIWLVpkP3/8+HGjYsWK9t8ZhnHuz+mDDz5of11UVGRUqVLFmD59epl8buBKxRxA/CONGjWy/+zp6anw8HA1bNjQ3hYRESFJ9ird66+/rv/+979KS0vT6dOnlZ+fr+uvv75Efe3Zs0cVKlTQDTfcYG+rXbu2QkND/zIuf39/BQUFOVQKSxLHtddeK09PT/vrqKgo7dq1q0SxAlbUokULh+kQcXFxmjBhgnbt2qXCwkJdc801Dtfn5eUpPDzc/rpixYqqVauW/XVUVNRFK/ySdPDgQRUUFOjGG2+0twUHBys2NrbYtX/+fWCz2RQZGXnJ+wJWQQKIf+TCFbo2m82h7fxfBkVFRXrvvfc0YsQITZgwQXFxcQoMDNQrr7yirVu3lktcRUVFklTiOP7qHgBKLjc3V56entq+fbvDP6okKSAgwP7zxf7MGWWw6pc/y0BxJIAoN5s2bVLLli31+OOP29sOHDhQ4vfHxsbq7NmzSk1NVZMmTSRJ+/fv1x9//FGucZzn7e2twsLCUr8PcFcX/iNqy5YtqlOnjho3bqzCwkJlZWXp5ptvLpO+atasKS8vL23btk3R0dGSpOzsbO3du1etW7cukz4Ad8YiEJSbOnXq6Ouvv9aKFSu0d+9ejRo1Stu2bSvx++vWrasOHTrokUce0VdffaXU1FQ98sgj8vPzK9Uq3H8ax3nVq1fXhg0b9Ntvv+n3338v9fsBd5OWlqakpCTt2bNHCxYs0NSpUzVkyBBdc801euCBB9SnTx8tWbJEhw4d0ldffaXk5GQtW7bssvoKDAxUQkKCnnjiCa1du1a7d+9W//795eHhwap8oARIAFFuHn30UcXHx6tnz55q3ry5jh496lCFK4m33npLERERat26te655x7961//UmBgoHx9fcs1DkkaO3asfvrpJ9WqVUuVK1cu9fsBd9OnTx+dPn1aN954oxITEzVkyBA98sgjkqQ5c+aoT58+Gj58uGJjY9W1a1eH6t3lmDhxouLi4nTnnXeqQ4cOatWqlerVq1eq3weAVdmMsphgAbjIr7/+qmrVqmnVqlW65ZZbXB0OABc6efKkrrrqKk2YMEH9+/d3dTiAqTEHEFeUNWvWKDc3Vw0bNlR6erqefPJJVa9enTk/gAWlpqbqxx9/1I033qjs7GyNHTtWktSlSxcXRwaYHwkgrigFBQV65plndPDgQQUGBqply5Z69913Tfu8YADO9eqrr2rPnj3y9vZWkyZN9MUXX6hSpUquDgswPYaAAQAALIZFIAAAABZDAggAAGAxJIAAAAAWQwIIAABgMSSAAAAAFkMCCMC0+vbtq65du9pft23bVkOHDi33ONatWyebzabjx4+Xe98A4AwkgABKrW/fvrLZbLLZbPL29lbt2rU1duxYnT171qn9LlmyRC+88EKJriVpA4BLYyNoAJfl9ttv15w5c5SXl6dPP/1UiYmJ8vLy0siRIx2uy8/Pl7e3d5n0GRYWVib3AQCrowII4LL4+PgoMjJSMTExGjBggDp06KCPPvrIPmz70ksvqWrVqoqNjZUk/fLLL+rRo4dCQkIUFhamLl266KeffrLfr7CwUElJSQoJCVF4eLiefPJJXbhP/YVDwHl5eXrqqadUrVo1+fj4qHbt2po9e7Z++ukntWvXTpIUGhoqm82mvn37SpKKioqUnJysGjVqyM/PT9ddd50WL17s0M+nn36qa665Rn5+fmrXrp1DnADgDkgAAZQJPz8/5efnS5JWr16tPXv2aOXKlfrkk09UUFCgjh07KjAwUF988YU2bdqkgIAA3X777fb3TJgwQXPnztV///tfbdy4UceOHdOHH374l3326dNHCxYs0JQpU/TDDz/ojTfeUEBAgKpVq6YPPvhAkrRnzx6lp6dr8uTJkqTk5GS99dZbmjFjhnbv3q1hw4bpwQcf1Pr16yWdS1Tj4+N11113aefOnXr44Yf19NNPO+trAwCXYAgYwD9iGIZWr16tFStWaNCgQTpy5Ij8/f01a9Ys+9DvO++8o6KiIs2aNUs2m02SNGfOHIWEhGjdunW67bbbNGnSJI0cOVLx8fGSpBkzZmjFihWX7Hfv3r1auHChVq5cqQ4dOkiSatasaT9/fri4SpUqCgkJkXSuYjhu3DitWrVKcXFx9vds3LhRb7zxhtq0aaPp06erVq1amjBhgiQpNjZWu3bt0n/+858y/NYAwLVIAAFclk8++UQBAQEqKChQUVGR7r//fo0ePVqJiYlq2LChw7y/b775Rvv371dgYKDDPc6cOaMDBw4oOztb6enpat68uf1chQoV1LRp02LDwOft3LlTnp6eatOmTYlj3r9/v06dOqVbb73VoT0/P1+NGzeWJP3www8OcUiyJ4sA4C5IAAFclnbt2mn69Ony9vZW1apVVaHC//068ff3d7g2NzdXTZo00bvvvlvsPpUrV76s/v38/Er9ntzcXEnSsmXLdNVVVzmc8/Hxuaw4AOBKRAII4LL4+/urdu3aJbr2hhtu0Pvvv68qVaooKCjootdERUVp69atat26tSTp7Nmz2r59u2644YaLXt+wYUMVFRVp/fr19iHgPztfgSwsLLS31a9fXz4+PkpLS7tk5bBevXr66KOPHNq2bNny9x8SAK4gLAIB4HQPPPCAKlWqpC5duuiLL77QoUOHtG7dOg0ePFi//vqrJGnIkCF6+eWXtXTpUv344496/PHH/3IPv+rVqyshIUEPPfSQli5dar/nwoULJUkxMTGy2Wz65JNPdOTIEeXm5iowMFAjRozQsGHDNG/ePB04cEA7duzQ1KlTNW/ePEnSY489pn379umJJ57Qnj17NH/+fM2dO9fZXxEAlCsSQABOV7FiRW3YsEHR0dGKj49XvXr11L9/f505c8ZeERw+fLh69+6thIQExcXFKTAwUPfcc89f3nf69Om699579fjjj6tu3br617/+pZMnT0qSrrrqKo0ZM0ZPP/20IiIiNHDgQEnSCy+8oFGjRik5OVn16tXT7bffrmXLlqlGjRqSpOjoaH3wwQdaunSprrvuOs2YMUPjxo1z4rcDAOXPZlxqhjUAAADcEhVAAAAAiyEBBAAAsBgSQAAAAIshAQQAALAYEkAAAACLIQEEAACwGBJAAAAAiyEBBAAAsBgSQAAAAIshAQQAALAYEkAAAACL+X8Mb8NcYbO8+AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary \n",
    "classification.\n",
    "'''\n",
    "'''\n",
    "Answer:-11\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.96\n",
      "Precision: 0.95\n",
      "Recall: 0.99\n",
      "F1-Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, \n",
    "Recall, and F1-Score.\n",
    "'''\n",
    "'''\n",
    "Answer:-12\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Calculate and print Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.86\n",
      "Precision: 0.41\n",
      "Recall: 0.75\n",
      "F1-Score: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to \n",
    "improve model performance.\n",
    "'''\n",
    "'''\n",
    "Answer:-13\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Generate an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression with class weights to handle imbalanced data\n",
    "model = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Calculate and print Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.81\n",
      "Precision: 0.79\n",
      "Recall: 0.74\n",
      "F1-Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and \n",
    "evaluate performance.\n",
    "'''\n",
    "'''\n",
    "Answer:-14\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Calculate and print Precision, Recall, and F1-Score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 0.79\n",
      "Accuracy with scaling: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression \n",
    "model. Evaluate its accuracy and compare results with and without scaling.\n",
    "'''\n",
    "'''\n",
    "Answer:-15\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression without scaling\n",
    "model_no_scaling = LogisticRegression(solver='liblinear')\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "print(f'Accuracy without scaling: {accuracy_no_scaling:.2f}')\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply Logistic Regression with scaling\n",
    "model_with_scaling = LogisticRegression(solver='liblinear')\n",
    "model_with_scaling.fit(X_train_scaled, y_train)\n",
    "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
    "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
    "print(f'Accuracy with scaling: {accuracy_with_scaling:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7483\n",
      "ROC-AUC Score: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
    "'''\n",
    "'''\n",
    "Answer:-16\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Convert categorical to numerical\n",
    "\n",
    "df = df[features + ['Survived']].dropna()  # Drop rows with missing values\n",
    "\n",
    "X = df[features]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Get probability scores for ROC-AUC\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with C=0.5: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate \n",
    "accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-17\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression with custom learning rate (C=0.5)\n",
    "model = LogisticRegression(C=0.5, solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy with C=0.5: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important Features based on Model Coefficients:\n",
      "    Feature  Coefficient\n",
      "1       Sex     2.592344\n",
      "5      Fare     0.003606\n",
      "2       Age    -0.025911\n",
      "4     Parch    -0.115246\n",
      "6  Embarked    -0.180969\n",
      "3     SibSp    -0.288863\n",
      "0    Pclass    -0.829730\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q18. Write a Python program to train Logistic Regression and identify important features based on model \n",
    "coefficients.\n",
    "'''\n",
    "'''\n",
    "Answer:-18\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Identify important features based on model coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print('Important Features based on Model Coefficients:')\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa \n",
    "Score.\n",
    "'''\n",
    "'''\n",
    "Answer:-19\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print Cohen's Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(f\"Cohen's Kappa Score: {kappa_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary \n",
    "classification.\n",
    "'''\n",
    "'''\n",
    "Answer:-20\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "average_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='b', lw=2, label=f'Precision-Recall curve (area = {average_precision:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with solver liblinear: 0.79\n",
      "Accuracy with solver saga: 0.79\n",
      "Accuracy with solver lbfgs: 0.81\n",
      "Comparison of solvers:\n",
      "Solver: liblinear, Accuracy: 0.79\n",
      "Solver: saga, Accuracy: 0.79\n",
      "Solver: lbfgs, Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare \n",
    "their accuracy.\n",
    "'''\n",
    "'''\n",
    "Answer:-21\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define solvers to compare\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "accuracy_results = {}\n",
    "\n",
    "# Train Logistic Regression with different solvers and compare accuracy\n",
    "for solver in solvers:\n",
    "    model = LogisticRegression(solver=solver, max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_results[solver] = accuracy\n",
    "    print(f'Accuracy with solver {solver}: {accuracy:.2f}')\n",
    "\n",
    "# Print the results\n",
    "print('Comparison of solvers:')\n",
    "for solver, accuracy in accuracy_results.items():\n",
    "    print(f'Solver: {solver}, Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.57\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews \n",
    "Correlation Coefficient (MCC).\n",
    "'''\n",
    "'''\n",
    "Answer:-22\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on raw data: 0.79\n",
      "Accuracy on standardized data: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their \n",
    "accuracy to see the impact of feature scaling.\n",
    "'''\n",
    "'''\n",
    "Answer:-23\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression on raw data\n",
    "model_raw = LogisticRegression(solver='liblinear')\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "print(f'Accuracy on raw data: {accuracy_raw:.2f}')\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression on standardized data\n",
    "model_scaled = LogisticRegression(solver='liblinear')\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f'Accuracy on standardized data: {accuracy_scaled:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1\n",
      "Best Cross-Validation Accuracy: 0.80\n",
      "Test Set Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using \n",
    "cross-validation.\n",
    "'''\n",
    "'''\n",
    "Answer:-24\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "# Handle missing values\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
    "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Prepare the data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for C (regularization strength)\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Apply GridSearchCV to find the optimal C\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameter and accuracy\n",
    "print(f'Best C: {grid_search.best_params_[\"C\"]}')\n",
    "print(f'Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Set Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7483\n",
      "ROC-AUC Score: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# Practical question\n",
    "'''\n",
    "Q25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to \n",
    "make predictions.\n",
    "'''\n",
    "'''\n",
    "Answer:-25\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# Select relevant features and target variable\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Convert categorical to numerical\n",
    "\n",
    "df = df[features + ['Survived']].dropna()  # Drop rows with missing values\n",
    "\n",
    "X = df[features]\n",
    "y = df['Survived']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, \"logistic_regression_model.pkl\")\n",
    "\n",
    "# Load the trained model\n",
    "loaded_model = joblib.load(\"logistic_regression_model.pkl\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "y_prob = loaded_model.predict_proba(X_test)[:, 1]  # Get probability scores for ROC-AUC\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
